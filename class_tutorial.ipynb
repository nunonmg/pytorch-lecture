{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Deep Structured Learning Class - March 29, 2022 - PyTorch Tutorial\n",
    "\n",
    "Some of the content in this class is inspired and adapted from the tutorials in _Dive Into Deep Learning_ ([https://d2l.ai/]), and PyTorch tutorials for this class in previous years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have worked with NumPy, then you will find this section familiar.\n",
    "A `Tensor` in both PyTorch is similar to NumPy's `ndarray` with\n",
    "a few killer features:\n",
    "* First, GPU is well-supported to accelerate the computation whereas NumPy only supports CPU computation.\n",
    "* Second, the tensor class supports automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "(**To start, we import `torch`.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "[**A tensor represents a (possibly multi-dimensional) array of numerical values.**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Let's start w/ a few functions for creating new tensors prepopulated with values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "(**We can access a tensor's *shape***) (the length along each axis)\n",
    "by inspecting its `shape` property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "If we just want to know the total number of elements in a tensor,\n",
    "i.e., the product of all of the shape elements,\n",
    "we can inspect its size.\n",
    "Because we are dealing with a vector here,\n",
    "the single element of its `shape` is identical to its size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "To [**change the shape of a tensor without altering\n",
    "either the number of elements or their values**],\n",
    "we can invoke the `reshape` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 21,
    "scrolled": true,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our target shape is a matrix with shape (height, width),\n",
    "then after we know the width, the height is given implicitly. We can use (-1) to represent the axis dimension that we want to remain fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 23
   },
   "outputs": [],
   "source": [
    "x.reshape(3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "Often, we want to [**randomly sample the values\n",
    "for each element in a tensor**]\n",
    "from some probability distribution.\n",
    "For example, when we construct arrays to serve\n",
    "as parameters in a neural network, we will\n",
    "typically initialize their values randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other possibilities for creating prepopulated tensors exist, such as `torch.ones(a,b)`, `torch.zeros(a, b)`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "We can also [**specify the exact values for each element**] in the desired tensor.\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "## Operations\n",
    "\n",
    "[**The common standard arithmetic operators\n",
    "(`+`, `-`, `*`, `/`, and `**`)\n",
    "have all been *lifted* to elementwise operations.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "Many (**more operations can be applied elementwise**),\n",
    "including unary operators like exponentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "We can also [***concatenate* multiple tensors together,**]\n",
    "stacking them end-to-end to form a larger tensor.\n",
    "We just need to provide a list of tensors\n",
    "and tell the system along which axis to concatenate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X = torch.arange(8, dtype=torch.float32).reshape((2,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 1], [1, 2, 3, 2]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "Sometimes, we want to [**construct a binary tensor via *logical statements*.**]\n",
    "Take `X == Y` as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53
   },
   "source": [
    "[**Summing all the elements in the tensor**] yields a tensor with only one element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 54,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 62
   },
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "[**`[-1]` selects the last element and `[:, 3:]`\n",
    "selects all elements along axis 0 (lines) and accesses the columns from the third onwards**] as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(8, dtype=torch.float32).reshape((2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 63,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X[-1], X[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Beyond reading, (**we can also write elements of a matrix by specifying indices.**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68
   },
   "source": [
    "If we want [**to assign multiple elements the same value,\n",
    "we simply index all of them and then assign them the value.**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 69,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X[0, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant example**: You will frequently have to mask parts of tensors when training/testing your models w/ batches of data. One particular example is that of masking some components of a vector prior to computing a probability distribution via softmax transformation (to do so, we assign a `-inf` value to those components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = F.softmax\n",
    "softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 3:] = -float(\"Inf\")\n",
    "softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 83
   },
   "source": [
    "## Conversion to Other Python Objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 85,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**Converting to a NumPy tensor (`ndarray`)**], or vice versa, is easy.\n",
    "Importantly, changing one through an in-place operation will also\n",
    "change the other!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 87,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 89
   },
   "source": [
    "To (**convert a size-1 tensor to a Python scalar**),\n",
    "we can invoke the `item` function or Python's built-in functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 91,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(8, dtype=torch.float32).reshape((2,4))\n",
    "[value.item() for value in X[0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra w/ Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with Vectors\n",
    "\n",
    "We can refer to any element of a vector by using a subscript.\n",
    "For example, we can refer to the $i^\\mathrm{th}$ element of $\\mathbf{x}$ by $x_i$.\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(3)\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We [**can access the length of a tensor**] or the [**the dimensionality along each axis of the tensor**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "We can [**create an $m \\times n$ matrix**]\n",
    "by specifying a shape with two components $m$ and $n$\n",
    "when calling any of our favorite functions for instantiating a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(9).reshape(3,3)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many methods exist, such as accessing a **matrix's transpose**, **matrix's main diagonal**, **matrix's trace**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(18, dtype=float).reshape(2,3,3)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Given any two tensors with the same shape,\n",
    "the result of any binary elementwise operation\n",
    "will be a tensor of that same shape.**]\n",
    "For example, adding two matrices of the same shape\n",
    "performs elementwise addition over these two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.clone()\n",
    "X + Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Elementwise multiplication of two matrices is called their *Hadamard product***] can be computed running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X * Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate [**the overall sum of their elements**], or the [**the sum along a given axis**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sum(axis=0), X.sum(axis=1), X.sum(axis=2), X.sum(axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.mean(axis=0), X.mean(axis=1), X.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X.sum(axis=1), X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X / X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant example**: Let us consider that we have two squared (3x3) matrices A and B, and we want to compute the row-wise dot product of A and B, and then the column-wise dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(9).reshape(-1,3)\n",
    "B = torch.arange(9).reshape(-1,3)\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(A*B, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(A*B, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Linear Algebra operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, the dot product is a sum over the products of the elements at the same position: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$. Given two 1D tensors, we can also compute it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4, dtype=float)\n",
    "y = torch.ones(4, dtype=float)\n",
    "x, y, torch.dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(8, dtype=float).reshape(4, -1)\n",
    "B = torch.ones(2, 3, dtype=float)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very intricate and somewhat complicated operation that may arise when implementing deep learning models is **batch matrix multiplication**. Thankfully, PyTorch has a function to help us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `input` is a $(b \\times n \\times m)$ tensor, `mat2` is a $(b \\times m \\times p)$ tensor, `out` will be a $(b \\times n \\times p)$ tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.arange(24, dtype=float).reshape(2, 4, 3)\n",
    "mat2 = torch.ones((2, 3, 5), dtype=float)\n",
    "torch.bmm(input, mat2), torch.bmm(input, mat2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, torch.norm(x), torch.abs(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, torch.norm(A), torch.norm(A, dim=0), torch.norm(A, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a little break now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automatic differentiation - Unleashing PyTorch's Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Now let us calculate $y$.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `x` is a vector of length 4,\n",
    "a dot product of `x` and `x` is performed,\n",
    "yielding the scalar output that we assign to `y`.\n",
    "Next, [**we can automatically calculate the gradient of `y`\n",
    "with respect to each component of `x`**]\n",
    "by calling the function for backpropagation and printing the gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaching Computation\n",
    "\n",
    "Sometimes, we wish to [**move some calculations\n",
    "outside of the recorded computational graph.**]\n",
    "For example, say that `y` was calculated as a function of `x`,\n",
    "and that subsequently `z` was calculated as a function of both `y` and `x`.\n",
    "Now, imagine that we wanted to calculate\n",
    "the gradient of `z` with respect to `x`,\n",
    "but wanted for some reason to treat `y` as a constant,\n",
    "and only take into account the role\n",
    "that `x` played after `y` was calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_() ## zero out the gradient\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_moons\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "X, y = make_moons(\n",
    "    n_samples=n_samples,\n",
    "    noise=0.01,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=25)\n",
    "ax.legend(handles=scatter.legend_elements()[0], labels=[0, 1] ,loc=\"best\")\n",
    "ax.set_title('Moon dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution w/ Numpy (w/o autodiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, n_features, n_targets=1, lr=0.01):\n",
    "        self.W = np.zeros((n_targets, n_features))\n",
    "        self.lr = lr\n",
    "\n",
    "    def update_weight(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        y_hat = self.predict(X)\n",
    "        W_grad = 2 * np.dot(X.T, y_hat - y) / m\n",
    "        self.W = self.W - self.lr * W_grad\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        return np.mean(np.power(y_hat - y, 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = np.dot(X, self.W.T)\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def train(self, X, y, epochs=50):\n",
    "        \"\"\"\n",
    "        X (n_examples x n_features):\n",
    "        y (n_examples): gold labels\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for _ in range(epochs):\n",
    "            # for x_i, y_i in zip(X, y):\n",
    "            #        self.update_weight(x_i, y_i)\n",
    "            self.update_weight(X, y)\n",
    "            y_hat = self.predict(X)\n",
    "            loss = self.loss(y_hat, y)\n",
    "            loss_history.append(loss)\n",
    "        return loss_history\n",
    "    \n",
    "use_bias = False\n",
    "if use_bias:\n",
    "    X_np = np.hstack([np.ones((n_samples,1)), X])\n",
    "    n_features += 1\n",
    "else:\n",
    "    X_np = X\n",
    "    \n",
    "model = LinearRegression(n_features=2, n_targets=1, lr=0.01)\n",
    "loss_history = model.train(X_np, y, epochs=100)\n",
    "y_hat = model.predict(X_np)\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss per epoch');\n",
    "\n",
    "# Vis\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,4))\n",
    "\n",
    "scatter = ax[0].scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=25)\n",
    "ax[0].legend(handles=scatter.legend_elements()[0], labels=[0, 1] ,loc=\"best\")\n",
    "ax[0].set_title('Moon dataset')\n",
    "\n",
    "sc = ax[1].scatter(X[:, 0], X[:, 1], marker=\"o\", c=y_hat, s=25)\n",
    "plt.colorbar(sc)\n",
    "ax[1].set_title(f\"MSE: {round(loss_history[-1],4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy using Autograd w/ Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedLinearRegression(object):\n",
    "    def __init__(self, n_features, n_targets=1, lr=0.01):\n",
    "        self.W = torch.zeros(n_targets, n_features, requires_grad=True)  # note requires_grad=True!\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update_weight(self):\n",
    "        # Gradients are given to us by autograd!\n",
    "        self.W.data = self.W.data - self.lr * self.W.grad.data\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        return torch.mean(torch.pow(y_hat - y, 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_hat = torch.matmul(X, self.W.t())\n",
    "        return y_hat.squeeze(-1)\n",
    "\n",
    "    def train(self, X, y, epochs=50):\n",
    "        \"\"\"\n",
    "        X (n_examples x n_features):\n",
    "        y (n_examples): gold labels\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for _ in range(epochs):\n",
    "            \n",
    "            # Our neural net is a Line function!\n",
    "            y_hat = self.predict(X)\n",
    "            \n",
    "            # Compute the loss using torch operations so they are saved in the gradient history.\n",
    "            loss = self.loss(y_hat, y)\n",
    "            \n",
    "            # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n",
    "            loss.backward()\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # Update a and b using gradient descent; a.data and b.data are Tensors.\n",
    "            self.update_weight()\n",
    "\n",
    "            # Reset the accumulated gradients\n",
    "            self.W.grad.data.zero_()\n",
    "            \n",
    "        return loss_history\n",
    "    \n",
    "X_pt = torch.from_numpy(X_np).float()\n",
    "y_pt = torch.from_numpy(y).float()\n",
    "\n",
    "model = MixedLinearRegression(n_features=2, n_targets=1, lr=0.01)\n",
    "loss_history = model.train(X_pt, y_pt, epochs=100)\n",
    "with torch.no_grad():\n",
    "    y_hat = model.predict(X_pt)\n",
    "    \n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss per epoch');\n",
    "\n",
    "# Vis\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,4))\n",
    "\n",
    "scatter = ax[0].scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=25)\n",
    "ax[0].legend(handles=scatter.legend_elements()[0], labels=[0, 1] ,loc=\"best\")\n",
    "ax[0].set_title('Moon dataset')\n",
    "\n",
    "sc = ax[1].scatter(X[:, 0], X[:, 1], marker=\"o\", c=y_hat, s=25)\n",
    "plt.colorbar(sc)\n",
    "ax[1].set_title(f\"MSE: {round(loss_history[-1],4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full PyTorch solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "X = torch.from_numpy(X).requires_grad_(True).float()\n",
    "y = torch.from_numpy(y).reshape((n_samples, 1)).float()\n",
    "\n",
    "class LinReg(nn.Module):\n",
    "    # Model definition. \n",
    "    # using off-the-shelf linear layer w/ bias.\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.beta(X)\n",
    "    \n",
    "# define model, loss function and optimizer\n",
    "\n",
    "# using off-the-shelf loss and optimizer.\n",
    "model = LinReg(input_dim=2).to(device)  # <-- here\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# move to GPU if available\n",
    "X, y = X.to(device), y.to(device)  # <-- here\n",
    "\n",
    "def train(model, X, y, epochs=50):\n",
    "    \"\"\"\n",
    "        Defining the training loop. \n",
    "    \"\"\"\n",
    "    model.train()  # <-- here\n",
    "    loss_history = []\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad() # sets the gradients \"to zero\".\n",
    "\n",
    "        y_ = model(X)\n",
    "        loss = loss_fn(y_, y)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        loss.backward() # computes the gradients.\n",
    "        optimizer.step() # updates weights using the gradients.\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "def evaluate(model, X):\n",
    "    \"\"\"\n",
    "        Evaluating the model. \n",
    "    \"\"\"\n",
    "    model.eval()  # <-- here\n",
    "    with torch.no_grad(): \n",
    "        y_ = model(X)    \n",
    "    return y_\n",
    "\n",
    "loss_history = train(model, X, y, epochs=100)\n",
    "y_hat = evaluate(model, X)\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss per epoch');\n",
    "\n",
    "# Vis\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,4))\n",
    "X_np = X.detach().numpy()\n",
    "scatter = ax[0].scatter(X_np[:, 0], X_np[:, 1], marker=\"o\", c=y, s=25)\n",
    "ax[0].legend(handles=scatter.legend_elements()[0], labels=[0, 1] ,loc=\"best\")\n",
    "ax[0].set_title('Moon dataset')\n",
    "\n",
    "sc = ax[1].scatter(X_np[:, 0], X_np[:, 1], marker=\"o\", c=y_hat, s=25)\n",
    "plt.colorbar(sc)\n",
    "ax[1].set_title(f\"MSE: {round(loss_history[-1],4)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Extending to a multi-layer perceptron is super easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(\n",
    "    n_samples=n_samples,\n",
    "    noise=0.01,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "X = torch.from_numpy(X).requires_grad_(True).float()\n",
    "y = torch.from_numpy(y).reshape((n_samples, 1)).float()\n",
    "\n",
    "class LinReg(nn.Module):\n",
    "    # Model definition. \n",
    "    # using off-the-shelf linear layer w/ bias.\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(input_dim, 3*input_dim)\n",
    "        self.W2 = nn.Linear(3*input_dim, 3*input_dim)\n",
    "        self.beta = nn.Linear(3*input_dim, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.beta(torch.tanh(self.W2(torch.tanh(self.W1(X)))))\n",
    "    \n",
    "# define model, loss function and optimizer\n",
    "\n",
    "# using off-the-shelf loss and optimizer.\n",
    "model = LinReg(input_dim=2).to(device)  # <-- here\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "# move to GPU if available\n",
    "X, y = X.to(device), y.to(device)  # <-- here\n",
    "\n",
    "def train(model, X, y, epochs=50):\n",
    "    \"\"\"\n",
    "        Defining the training loop. \n",
    "    \"\"\"\n",
    "    model.train()  # <-- here\n",
    "    loss_history = []\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad() # sets the gradients \"to zero\".\n",
    "\n",
    "        y_ = model(X)\n",
    "        loss = loss_fn(y_, y)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        loss.backward() # computes the gradients.\n",
    "        optimizer.step() # updates weights using the gradients.\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "def evaluate(model, X):\n",
    "    \"\"\"\n",
    "        Evaluating the model. \n",
    "    \"\"\"\n",
    "    model.eval()  # <-- here\n",
    "    with torch.no_grad(): \n",
    "        y_ = model(X)    \n",
    "    return y_\n",
    "\n",
    "loss_history = train(model, X, y, epochs=2000)\n",
    "y_hat = evaluate(model, X)\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss per epoch');\n",
    "\n",
    "# Vis\n",
    "fig, ax = plt.subplots(1,2, figsize=(14,4))\n",
    "X_np = X.detach().numpy()\n",
    "scatter = ax[0].scatter(X_np[:, 0], X_np[:, 1], marker=\"o\", c=y, s=25)\n",
    "ax[0].legend(handles=scatter.legend_elements()[0], labels=[0, 1] ,loc=\"best\")\n",
    "ax[0].set_title('Moon dataset')\n",
    "\n",
    "sc = ax[1].scatter(X_np[:, 0], X_np[:, 1], marker=\"o\", c=y_hat, s=25)\n",
    "plt.colorbar(sc)\n",
    "ax[1].set_title(f\"MSE: {round(loss_history[-1],5)}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
